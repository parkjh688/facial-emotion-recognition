{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fer2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "* emotion label (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n",
    "* 48 X 48 pixels \n",
    "* The training set consists of 28,709 examples. <br>The public test set used for the leaderboard consists of 3,589 examples.<br>The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = data['pixels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "# faces = np.array([])\n",
    "for pixel_row in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_row.split(' ')]\n",
    "    face = np.asarray(face).reshape(48, 48) # pixel size\n",
    "    faces.append(face.astype('float32'))\n",
    "#     faces = np.append(faces, face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = np.asarray(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 70.,  80.,  82., ...,  52.,  43.,  41.],\n",
       "        [ 65.,  61.,  58., ...,  56.,  52.,  44.],\n",
       "        [ 50.,  43.,  54., ...,  49.,  56.,  47.],\n",
       "        ...,\n",
       "        [ 91.,  65.,  42., ...,  72.,  56.,  43.],\n",
       "        [ 77.,  82.,  79., ..., 105.,  70.,  46.],\n",
       "        [ 77.,  72.,  84., ..., 106., 109.,  82.]],\n",
       "\n",
       "       [[151., 150., 147., ..., 129., 140., 120.],\n",
       "        [151., 149., 149., ..., 122., 141., 137.],\n",
       "        [151., 151., 156., ..., 109., 123., 146.],\n",
       "        ...,\n",
       "        [188., 188., 121., ..., 185., 185., 186.],\n",
       "        [188., 187., 196., ..., 186., 182., 187.],\n",
       "        [186., 184., 185., ..., 193., 183., 184.]],\n",
       "\n",
       "       [[231., 212., 156., ...,  44.,  27.,  16.],\n",
       "        [229., 175., 148., ...,  27.,  35.,  27.],\n",
       "        [214., 156., 157., ...,  28.,  22.,  28.],\n",
       "        ...,\n",
       "        [241., 245., 250., ...,  57., 101., 146.],\n",
       "        [246., 250., 252., ...,  78., 105., 162.],\n",
       "        [250., 251., 250., ...,  88., 110., 152.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 17.,  17.,  16., ...,  83., 114., 245.],\n",
       "        [ 18.,  17.,  16., ..., 104., 136., 253.],\n",
       "        [ 19.,  16.,  17., ..., 128., 152., 255.],\n",
       "        ...,\n",
       "        [  4.,  21.,  46., ..., 186., 180., 187.],\n",
       "        [  5.,  17.,  41., ..., 177., 172., 176.],\n",
       "        [ 20.,  15.,  22., ..., 154., 133., 113.]],\n",
       "\n",
       "       [[ 30.,  28.,  28., ...,  60.,  50.,  44.],\n",
       "        [ 30.,  27.,  28., ...,  64.,  52.,  40.],\n",
       "        [ 31.,  28.,  30., ...,  61.,  54.,  37.],\n",
       "        ...,\n",
       "        [104., 109., 110., ...,  35.,  30.,  30.],\n",
       "        [102., 105., 108., ...,  35.,  31.,  29.],\n",
       "        [ 93.,  96., 100., ...,  35.,  30.,  28.]],\n",
       "\n",
       "       [[ 19.,  13.,  14., ..., 108.,  95.,  86.],\n",
       "        [ 16.,  17.,  15., ..., 105.,  94.,  90.],\n",
       "        [ 10.,   9.,  10., ..., 101.,  93.,  95.],\n",
       "        ...,\n",
       "        [ 18.,  14.,  16., ...,  55.,  64.,  95.],\n",
       "        [ 15.,  15.,  13., ..., 123., 171., 192.],\n",
       "        [ 16.,  14.,  13., ..., 189., 199., 201.]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,  80.,  82., ...,  52.,  43.,  41.],\n",
       "       [ 65.,  61.,  58., ...,  56.,  52.,  44.],\n",
       "       [ 50.,  43.,  54., ...,  49.,  56.,  47.],\n",
       "       ...,\n",
       "       [ 91.,  65.,  42., ...,  72.,  56.,  43.],\n",
       "       [ 77.,  82.,  79., ..., 105.,  70.,  46.],\n",
       "       [ 77.,  72.,  84., ..., 106., 109.,  82.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115c58860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(faces[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/facial-emotion-recognition/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "faces = np.asarray(faces)\n",
    "faces = np.expand_dims(faces, -1) \n",
    "\n",
    "emotions = pd.get_dummies(data['emotion']).as_matrix() # one - hot encoding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "width, height = 48, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    " \n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        25632     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 48, 48, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,104,327\n",
      "Trainable params: 1,104,327\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, \n",
    "             optimizer=Adam(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./borad/sample_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_path = '{epoch:02d}-{val_acc:.2f}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(check_path, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29068 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "29068/29068 [==============================] - 685s 24ms/step - loss: 3.8690 - acc: 0.2563 - val_loss: 2.3266 - val_acc: 0.2388\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.32662, saving model to 01-0.24.h5\n",
      "Epoch 2/100\n",
      "29068/29068 [==============================] - 682s 23ms/step - loss: 2.4087 - acc: 0.3151 - val_loss: 2.1045 - val_acc: 0.3394\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.32662 to 2.10454, saving model to 02-0.34.h5\n",
      "Epoch 3/100\n",
      "29068/29068 [==============================] - 658s 23ms/step - loss: 2.2684 - acc: 0.3448 - val_loss: 1.9745 - val_acc: 0.3848\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.10454 to 1.97452, saving model to 03-0.38.h5\n",
      "Epoch 4/100\n",
      "29068/29068 [==============================] - 636s 22ms/step - loss: 2.2634 - acc: 0.3594 - val_loss: 2.2275 - val_acc: 0.3851\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.97452\n",
      "Epoch 5/100\n",
      "29068/29068 [==============================] - 635s 22ms/step - loss: 2.1960 - acc: 0.3766 - val_loss: 1.9621 - val_acc: 0.3937\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.97452 to 1.96215, saving model to 05-0.39.h5\n",
      "Epoch 6/100\n",
      "29068/29068 [==============================] - 630s 22ms/step - loss: 2.1166 - acc: 0.3871 - val_loss: 1.9797 - val_acc: 0.4007\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.96215\n",
      "Epoch 7/100\n",
      "29068/29068 [==============================] - 678s 23ms/step - loss: 2.1969 - acc: 0.3824 - val_loss: 1.8941 - val_acc: 0.4266\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.96215 to 1.89414, saving model to 07-0.43.h5\n",
      "Epoch 8/100\n",
      "29068/29068 [==============================] - 732s 25ms/step - loss: 2.0506 - acc: 0.4041 - val_loss: 2.1086 - val_acc: 0.3870\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.89414\n",
      "Epoch 9/100\n",
      "29068/29068 [==============================] - 647s 22ms/step - loss: 2.0357 - acc: 0.4047 - val_loss: 1.8799 - val_acc: 0.4235\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.89414 to 1.87993, saving model to 09-0.42.h5\n",
      "Epoch 10/100\n",
      "29068/29068 [==============================] - 667s 23ms/step - loss: 2.0580 - acc: 0.4074 - val_loss: 1.7244 - val_acc: 0.3993\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.87993 to 1.72440, saving model to 10-0.40.h5\n",
      "Epoch 11/100\n",
      "29068/29068 [==============================] - 697s 24ms/step - loss: 2.0187 - acc: 0.4133 - val_loss: 1.7977 - val_acc: 0.4408\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.72440\n",
      "Epoch 12/100\n",
      "29068/29068 [==============================] - 724s 25ms/step - loss: 1.9375 - acc: 0.4320 - val_loss: 1.7816 - val_acc: 0.4547\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.72440\n",
      "Epoch 13/100\n",
      "29068/29068 [==============================] - 715s 25ms/step - loss: 1.9292 - acc: 0.4332 - val_loss: 1.7608 - val_acc: 0.4592\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.72440\n",
      "Epoch 14/100\n",
      "29068/29068 [==============================] - 729s 25ms/step - loss: 1.8997 - acc: 0.4295 - val_loss: 1.6359 - val_acc: 0.4553\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.72440 to 1.63587, saving model to 14-0.46.h5\n",
      "Epoch 15/100\n",
      "29068/29068 [==============================] - 699s 24ms/step - loss: 1.9257 - acc: 0.4339 - val_loss: 1.6368 - val_acc: 0.4531\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.63587\n",
      "Epoch 16/100\n",
      "29068/29068 [==============================] - 684s 24ms/step - loss: 1.8688 - acc: 0.4443 - val_loss: 1.6162 - val_acc: 0.4876\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.63587 to 1.61619, saving model to 16-0.49.h5\n",
      "Epoch 17/100\n",
      "29068/29068 [==============================] - 714s 25ms/step - loss: 1.8411 - acc: 0.4509 - val_loss: 1.6869 - val_acc: 0.4681\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.61619\n",
      "Epoch 18/100\n",
      "29068/29068 [==============================] - 729s 25ms/step - loss: 1.8068 - acc: 0.4538 - val_loss: 1.6058 - val_acc: 0.4737\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.61619 to 1.60583, saving model to 18-0.47.h5\n",
      "Epoch 19/100\n",
      "29068/29068 [==============================] - 634s 22ms/step - loss: 1.8280 - acc: 0.4550 - val_loss: 1.5868 - val_acc: 0.4943\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.60583 to 1.58678, saving model to 19-0.49.h5\n",
      "Epoch 20/100\n",
      "29068/29068 [==============================] - 613s 21ms/step - loss: 1.8063 - acc: 0.4573 - val_loss: 1.6474 - val_acc: 0.4937\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.58678\n",
      "Epoch 21/100\n",
      "29068/29068 [==============================] - 618s 21ms/step - loss: 1.8079 - acc: 0.4572 - val_loss: 1.5769 - val_acc: 0.4851\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.58678 to 1.57687, saving model to 21-0.49.h5\n",
      "Epoch 22/100\n",
      "29068/29068 [==============================] - 613s 21ms/step - loss: 1.7520 - acc: 0.4657 - val_loss: 1.6721 - val_acc: 0.4948\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.57687\n",
      "Epoch 23/100\n",
      "29068/29068 [==============================] - 611s 21ms/step - loss: 1.8386 - acc: 0.4501 - val_loss: 1.6389 - val_acc: 0.4968\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.57687\n",
      "Epoch 24/100\n",
      "29068/29068 [==============================] - 614s 21ms/step - loss: 1.7878 - acc: 0.4642 - val_loss: 1.6007 - val_acc: 0.4890\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.57687\n",
      "Epoch 25/100\n",
      "29068/29068 [==============================] - 614s 21ms/step - loss: 1.7469 - acc: 0.4712 - val_loss: 1.5313 - val_acc: 0.5024\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.57687 to 1.53135, saving model to 25-0.50.h5\n",
      "Epoch 26/100\n",
      "29068/29068 [==============================] - 614s 21ms/step - loss: 1.7670 - acc: 0.4701 - val_loss: 1.5503 - val_acc: 0.5085\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.53135\n",
      "Epoch 27/100\n",
      "29068/29068 [==============================] - 615s 21ms/step - loss: 1.7544 - acc: 0.4701 - val_loss: 1.5377 - val_acc: 0.4890\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.53135\n",
      "Epoch 28/100\n",
      "29068/29068 [==============================] - 615s 21ms/step - loss: 1.7368 - acc: 0.4699 - val_loss: 1.5934 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.53135\n",
      "Epoch 29/100\n",
      "29068/29068 [==============================] - 618s 21ms/step - loss: 1.7340 - acc: 0.4769 - val_loss: 1.4593 - val_acc: 0.5024\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.53135 to 1.45930, saving model to 29-0.50.h5\n",
      "Epoch 30/100\n",
      "29068/29068 [==============================] - 619s 21ms/step - loss: 1.7109 - acc: 0.4758 - val_loss: 1.4762 - val_acc: 0.5107\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.45930\n",
      "Epoch 31/100\n",
      "29068/29068 [==============================] - 617s 21ms/step - loss: 1.7274 - acc: 0.4756 - val_loss: 1.4302 - val_acc: 0.5110\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.45930 to 1.43016, saving model to 31-0.51.h5\n",
      "Epoch 32/100\n",
      "29068/29068 [==============================] - 618s 21ms/step - loss: 1.7185 - acc: 0.4807 - val_loss: 1.5017 - val_acc: 0.4868\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.43016\n",
      "Epoch 33/100\n",
      "29068/29068 [==============================] - 618s 21ms/step - loss: 1.6942 - acc: 0.4799 - val_loss: 1.5131 - val_acc: 0.5035\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.43016\n",
      "Epoch 34/100\n",
      "29068/29068 [==============================] - 617s 21ms/step - loss: 1.6563 - acc: 0.4838 - val_loss: 1.5745 - val_acc: 0.5082\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.43016\n",
      "Epoch 35/100\n",
      "29068/29068 [==============================] - 616s 21ms/step - loss: 1.6875 - acc: 0.4848 - val_loss: 1.5541 - val_acc: 0.5127\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.43016\n",
      "Epoch 36/100\n",
      "29068/29068 [==============================] - 616s 21ms/step - loss: 1.6717 - acc: 0.4893 - val_loss: 1.5043 - val_acc: 0.5171\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.43016\n",
      "Epoch 37/100\n",
      "29068/29068 [==============================] - 621s 21ms/step - loss: 1.6655 - acc: 0.4859 - val_loss: 1.4837 - val_acc: 0.5194\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.43016\n",
      "Epoch 38/100\n",
      "29068/29068 [==============================] - 622s 21ms/step - loss: 1.6721 - acc: 0.4911 - val_loss: 1.4762 - val_acc: 0.5107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_loss did not improve from 1.43016\n",
      "Epoch 39/100\n",
      "29068/29068 [==============================] - 620s 21ms/step - loss: 1.6666 - acc: 0.4904 - val_loss: 1.4420 - val_acc: 0.5091\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.43016\n",
      "Epoch 00039: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x115c81cc0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), \n",
    "          batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_data=(np.array(X_test), np.array(y_test)),\n",
    "         shuffle=True,\n",
    "         callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    " \n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    " \n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 32263     \n",
      "=================================================================\n",
      "Total params: 124,935\n",
      "Trainable params: 124,935\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/facial-emotion-recognition/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 48, 48)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (28706, 48, 48)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-4191ab1f8777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m          callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/facial-emotion-recognition/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/facial-emotion-recognition/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/facial-emotion-recognition/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (28706, 48, 48)"
     ]
    }
   ],
   "source": [
    "for index, (train_indices, val_indices) in enumerate(skf.split(faces, data['emotion'])):\n",
    "    # Generate batches from indices\n",
    "    X_train, X_val = faces[train_indices], faces[val_indices]\n",
    "    y_train, y_val = emotions[train_indices], emotions[val_indices]\n",
    "    # Clear model, and create it\n",
    "    model.fit(np.array(X_train), np.array(y_train), \n",
    "          batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_data=(np.array(X_val), np.array(y_val)),\n",
    "         shuffle=True,\n",
    "         callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
