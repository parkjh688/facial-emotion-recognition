{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fer2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "* emotion label (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n",
    "* 48 X 48 pixels \n",
    "* The training set consists of 28,709 examples. <br>The public test set used for the leaderboard consists of 3,589 examples.<br>The final test set, which was used to determine the winner of the competition, consists of another 3,589 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        Usage\n",
       "35882        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35883        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35884        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35885        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35886        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = data['pixels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = []\n",
    "for pixel_row in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_row.split(' ')]\n",
    "    face = np.asarray(face).reshape(48, 48) # pixel size\n",
    "    faces.append(face.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70.,  80.,  82., ...,  52.,  43.,  41.],\n",
       "       [ 65.,  61.,  58., ...,  56.,  52.,  44.],\n",
       "       [ 50.,  43.,  54., ...,  49.,  56.,  47.],\n",
       "       ...,\n",
       "       [ 91.,  65.,  42., ...,  72.,  56.,  43.],\n",
       "       [ 77.,  82.,  79., ..., 105.,  70.,  46.],\n",
       "       [ 77.,  72.,  84., ..., 106., 109.,  82.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11202bbe0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(faces[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/facial-recognition/lib/python3.5/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "faces = np.asarray(faces)\n",
    "faces = np.expand_dims(faces, -1) \n",
    "\n",
    "emotions = pd.get_dummies(data['emotion']).as_matrix() # one - hot encoding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(faces, emotions, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "width, height = 48, 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    " \n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2*2*2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2*2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 48, 48, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 48, 48, 32)        25632     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 1,104,198\n",
      "Trainable params: 1,104,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy, \n",
    "             optimizer=Adam(),\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=8, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint('/model', monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (6,) but got array with shape (7,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7db11c9337e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m          callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/facial-recognition/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/facial-recognition/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/facial-recognition/lib/python3.5/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (6,) but got array with shape (7,)"
     ]
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), \n",
    "          batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_data=(np.array(X_test), np.array(y_test)),\n",
    "         shuffle=True,\n",
    "         callbacks=[lr_reducer, tensorboard, early_stopper, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
